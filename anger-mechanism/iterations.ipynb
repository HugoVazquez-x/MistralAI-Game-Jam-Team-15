{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Determining the \"best\" French cheese can be quite subjective, as it depends on personal taste. France is renowned for its wide variety of cheeses, with over 1,000 different types. Here are a few highly regarded French cheeses across various categories:\n",
      "\n",
      "1. **Soft Cheeses**:\n",
      "   - **Brie de Meaux**: Often referred to as the \"King of Cheeses,\" it is known for its rich, creamy interior and delicate taste.\n",
      "   - **Camembert de Normandie**: Another famous soft cheese with a smooth, creamy interior and a white bloomy rind.\n",
      "\n",
      "2. **Semi-Soft Cheeses**:\n",
      "   - **Reblochon**: A savory cheese from the Alps with a nutty flavor and a soft, washed rind.\n",
      "   - **Munster**: A strong-tasting cheese with a washed rind, typically from the Vosges region.\n",
      "\n",
      "3. **Hard Cheeses**:\n",
      "   - **Comté**: A popular hard cheese made from unpasteurized cow’s milk, known for its complex flavors that range from sweet to nutty.\n",
      "   - **Beaufort**: A firm, raw cow's milk cheese with a slightly grainy texture and a rich, nutty flavor.\n",
      "\n",
      "4. **Blue Cheeses**:\n",
      "   - **Roquefort**: A famous blue cheese made from sheep milk, known for its strong, tangy flavor and crumbly texture.\n",
      "   - **Bleu d'Auvergne**: A milder blue cheese with a creamy texture and a delicate flavor.\n",
      "\n",
      "5. **Goat Cheeses**:\n",
      "   - **Sainte-Maure de Touraine**: A log-shaped goat cheese with a distinctive taste and a slight tang.\n",
      "   - **Crottin de Chavignol**: A small, cylindrical goat cheese with a firm texture and a range of flavors from mild to strong.\n",
      "\n",
      "Each of these cheeses has its own unique characteristics and is beloved for different reasons. The best way to find your favorite is to try a variety and see which ones you enjoy the most!\n"
     ]
    }
   ],
   "source": [
    "from mistralai import Mistral\n",
    "\n",
    "MISTRAL_KEY = \"MaFKaY7R8TYrOLd56ZDRBcUfatOrebRh\"\n",
    "\n",
    "model = \"mistral-large-latest\"\n",
    "\n",
    "# Initialize the Mistral client\n",
    "client = Mistral(api_key=MISTRAL_KEY)\n",
    "\n",
    "# Create a chat completion\n",
    "chat_response = client.chat.complete(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is the best French cheese?\",\n",
    "        },\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Print the assistant's response\n",
    "print(chat_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'ddg' from 'duckduckgo_search' (/Users/raphaeljean/Documents/hackaton_mistral/.venv/lib/python3.13/site-packages/duckduckgo_search/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mduckduckgo_search\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ddg\n\u001b[1;32m      3\u001b[0m results \u001b[38;5;241m=\u001b[39m ddg(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m, region\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfr-fr\u001b[39m\u001b[38;5;124m'\u001b[39m, safesearch\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModerate\u001b[39m\u001b[38;5;124m'\u001b[39m, time\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m, max_results\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(results)\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'ddg' from 'duckduckgo_search' (/Users/raphaeljean/Documents/hackaton_mistral/.venv/lib/python3.13/site-packages/duckduckgo_search/__init__.py)"
     ]
    }
   ],
   "source": [
    "from duckduckgo_search import ddg\n",
    "\n",
    "results = ddg(\"test\", region='fr-fr', safesearch='Moderate', time='y', max_results=10)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL trouvée : https://francevoyager.com/best-french-cheeses/\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "403 Client Error: Forbidden for url: https://francevoyager.com/best-french-cheeses/",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 76\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mURL trouvée : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfound_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# 3. On scrape la page\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m website_text \u001b[38;5;241m=\u001b[39m \u001b[43mscrape_website\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfound_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m website_text:\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAucun texte extrait du site.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[15], line 29\u001b[0m, in \u001b[0;36mscrape_website\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124;03mRécupère le contenu HTML à l'URL spécifiée,\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124;03mextrait le texte principal et le renvoie sous forme de chaîne.\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     28\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(url)\n\u001b[0;32m---> 29\u001b[0m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m html_content \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Parser le HTML avec BeautifulSoup\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/hackaton_mistral/.venv/lib/python3.13/site-packages/requests/models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1019\u001b[0m     http_error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1020\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Server Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreason\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1021\u001b[0m     )\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 403 Client Error: Forbidden for url: https://francevoyager.com/best-french-cheeses/"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from duckduckgo_search import DDGS  # Utilisation de la classe DDGS\n",
    "\n",
    "from mistralai import Mistral\n",
    "\n",
    "# Clé et nom de modèle à adapter\n",
    "MISTRAL_KEY = \"MaFKaY7R8TYrOLd56ZDRBcUfatOrebRh\"\n",
    "MODEL_NAME = \"mistral-large-latest\"\n",
    "\n",
    "def search_url(query, region='fr-FR', max_results=5):\n",
    "    \"\"\"\n",
    "    Effectue une recherche sur DuckDuckGo en utilisant la classe DDGS et retourne la première URL trouvée.\n",
    "    \"\"\"\n",
    "    with DDGS() as ddgs:\n",
    "        results = ddgs.text(query, region=region, safesearch=\"Moderate\", timelimit=\"y\")\n",
    "        for i, result in enumerate(results):\n",
    "            if i >= max_results:\n",
    "                break\n",
    "            return result[\"href\"]  # Retourne le lien de la première réponse\n",
    "    return None\n",
    "\n",
    "def scrape_website(url):\n",
    "    \"\"\"\n",
    "    Récupère le contenu HTML à l'URL spécifiée,\n",
    "    extrait le texte principal et le renvoie sous forme de chaîne.\n",
    "    \"\"\"\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    html_content = response.text\n",
    "\n",
    "    # Parser le HTML avec BeautifulSoup\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "    # Extraire tout le texte\n",
    "    paragraphs = soup.find_all(\"p\")\n",
    "    extracted_text = \"\\n\".join(p.get_text() for p in paragraphs)\n",
    "\n",
    "    return extracted_text\n",
    "\n",
    "def create_mistral_client(api_key: str):\n",
    "    \"\"\"\n",
    "    Initialise et renvoie un client Mistral.\n",
    "    \"\"\"\n",
    "    return Mistral(api_key=api_key)\n",
    "\n",
    "def ask_mistral(client, model_name, user_content):\n",
    "    \"\"\"\n",
    "    Envoie une requête \"chat completion\" au modèle Mistral avec un message utilisateur.\n",
    "    Renvoie la réponse du modèle.\n",
    "    \"\"\"\n",
    "    response = client.chat.complete(\n",
    "        model=model_name,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": user_content\n",
    "            },\n",
    "        ]\n",
    "    )\n",
    "    # On suppose ici qu'il y a au moins une choice\n",
    "    return response[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. On prépare la question de recherche\n",
    "    user_query = \"Best French cheese\"\n",
    "\n",
    "    # 2. On fait la recherche DuckDuckGo\n",
    "    found_url = search_url(user_query)\n",
    "    if not found_url:\n",
    "        print(\"Aucun résultat trouvé pour votre recherche.\")\n",
    "        exit()\n",
    "\n",
    "    print(f\"URL trouvée : {found_url}\")\n",
    "\n",
    "    # 3. On scrape la page\n",
    "    website_text = scrape_website(found_url)\n",
    "    if not website_text:\n",
    "        print(\"Aucun texte extrait du site.\")\n",
    "        exit()\n",
    "\n",
    "    # 4. On interroge Mistral\n",
    "    mistral_client = create_mistral_client(MISTRAL_KEY)\n",
    "\n",
    "    # Facultatif : tronquer ou résumer le texte si trop volumineux\n",
    "    max_length = 2000  # Ajustez selon les capacités du modèle / votre prompt\n",
    "    truncated_text = website_text[:max_length]\n",
    "\n",
    "    user_prompt = (\n",
    "        \"Voici un texte récupéré depuis une page web trouvée en cherchant \"\n",
    "        f\"'{user_query}':\\n\\n\"\n",
    "        f\"{truncated_text}\\n\\n\"\n",
    "        \"Peux-tu me donner une synthèse ou une réponse à la question : \"\n",
    "        \"'Quel est le meilleur fromage français ?'\"\n",
    "    )\n",
    "\n",
    "    answer = ask_mistral(mistral_client, MODEL_NAME, user_prompt)\n",
    "\n",
    "    # 5. On affiche la réponse\n",
    "    print(\"----- RÉPONSE DU MODÈLE MISTRAL -----\")\n",
    "    print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "MODEL = \"ministral-8b-latest\"\n",
    "\n",
    "def link_with_newline_middle(strings):\n",
    "    if len(strings) <= 1:\n",
    "        return ''.join(strings)  # Return as is for single or empty lists\n",
    "    middle = len(strings) // 2\n",
    "    # Join the first part, add '\\n', and then join the rest\n",
    "    return '\\n'.join([' '.join(strings[:middle]), ' '.join(strings[middle:])])\n",
    "\n",
    "\n",
    "class AIAgent:\n",
    "    def __init__(self, name, character, emotions, goal, general_context, arbitrary_agent):\n",
    "        \"\"\"\n",
    "        Initialise l'agent IA avec ses attributs de base.\n",
    "\n",
    "        :param character: Dictionnaire décrivant la personnalité de l'IA (par ex. calme, agressif, analytique).\n",
    "        :param emotions: Dictionnaire des émotions actuelles de l'IA.\n",
    "        :param environment: Dictionnaire décrivant l'état actuel de l'environnement.\n",
    "        \"\"\"\n",
    "        self.name = name\n",
    "        self.character = character\n",
    "        self.emotions = emotions\n",
    "        self.goal = goal\n",
    "        self.general_context = general_context\n",
    "        self.arbitrary_agent = arbitrary_agent\n",
    "        self.conversation_history = []\n",
    "\n",
    "    def respond(self, input_text, opponent_state):\n",
    "        \"\"\"\n",
    "        Génère une réponse basée sur le contexte fourni.\n",
    "\n",
    "        :param input_text: Texte reçu de l'utilisateur ou de l'adversaire.\n",
    "        :param opponent_state: Dictionnaire décrivant l'état actuel de l'adversaire.\n",
    "        :return: Réponse de l'agent IA.\n",
    "        \"\"\"\n",
    "        # Ajouter le texte à l'historique de la conversation\n",
    "        self.conversation_history.append({\"user\": input_text})\n",
    "\n",
    "        # Générer une réponse basée sur les différents contextes\n",
    "        response = self._generate_response(input_text, opponent_state)\n",
    "\n",
    "        # Ajouter la réponse de l'IA à l'historique\n",
    "        self.conversation_history.append({self.name: response})\n",
    "\n",
    "        return response\n",
    "\n",
    "    def update_emotions(self, emotions):\n",
    "        arbitrary_agent = self.arbitrary_agent\n",
    "\n",
    "\n",
    "    def _generate_response(self, instructions, opponent_state, environment_description, max_tokens = None):\n",
    "        # Structure the messages list correctly\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": (\n",
    "                    f\"General context: {self.general_context}\\n\"\n",
    "                    f\"Character: {self.character}\\n\"\n",
    "                    f\"Goal: {self.goal}\\n\"\n",
    "                    f\"Emotions: {self.emotions}\\n\"\n",
    "                    f\"Environment: {environment_description}\\n\"\n",
    "                ),\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": (\n",
    "                    f\"Instructions: {instructions}\\n\"\n",
    "                    f\"Opponent state: {opponent_state}\\n\"\n",
    "                    f\"Conversation history: {self.conversation_history}\"\n",
    "                ),\n",
    "            },\n",
    "        ]\n",
    "\n",
    "        # Call the chat completion API\n",
    "        chat_response = client.chat.complete(\n",
    "            model=MODEL,\n",
    "            messages=messages,\n",
    "    max_tokens=max_tokens,  # Adjust this value as needed\n",
    ")\n",
    "        \n",
    "        # Extract and return the assistant's response\n",
    "        return chat_response.choices[0].message.content\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General context for the debate\n",
    "general_context = {\n",
    "    \"topic\": \"Economic Policy\",\n",
    "    \"setting\": \"Televised national debate\",\n",
    "    \"audience\": \"Undecided voters seeking clarity on candidates' economic plans\"\n",
    "}\n",
    "\n",
    "# Attributes for Donald Trump\n",
    "trump_character = {\n",
    "    \"dominant\": True,\n",
    "    \"charismatic\": True,\n",
    "    \"unpredictable\": True,\n",
    "    \"assertive\": True,\n",
    "    \"narcissistic\": True\n",
    "}\n",
    "\n",
    "trump_emotions = {\n",
    "    \"confidence\": 0.9,\n",
    "    \"aggressiveness\": 0.8,\n",
    "    \"enthusiasm\": 0.7\n",
    "}\n",
    "\n",
    "trump_goal = \"Persuade the audience that his economic policies will lead to national prosperity and job growth.\"\n",
    "\n",
    "# Attributes for Kamala Harris\n",
    "harris_character = {\n",
    "    \"analytical\": True,\n",
    "    \"empathetic\": True,\n",
    "    \"methodical\": True,\n",
    "    \"decisive\": True,\n",
    "    \"direct\": True\n",
    "}\n",
    "\n",
    "harris_emotions = {\n",
    "    \"confidence\": 0.85,\n",
    "    \"determination\": 0.9,\n",
    "    \"calmness\": 0.8\n",
    "}\n",
    "\n",
    "harris_goal = \"Convince the audience that her economic plans offer sustainable growth and address income inequality.\"\n",
    "\n",
    "# Creating the AI agent instances\n",
    "trump_agent = AIAgent(\n",
    "    name=\"Donald Trump\",\n",
    "    character=trump_character,\n",
    "    emotions=trump_emotions,\n",
    "    goal=trump_goal,\n",
    "    general_context=general_context\n",
    ")\n",
    "\n",
    "harris_agent = AIAgent(\n",
    "    name=\"Kamala Harris\",\n",
    "    character=harris_character,\n",
    "    emotions=harris_emotions,\n",
    "    goal=harris_goal,\n",
    "    general_context=general_context\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Tour de débat n°1\n",
      "==================================================\n",
      "\n",
      "[MODERATOR] Question: How will your policies affect small businesses?\n",
      "\n",
      "[Donald Trump] Réponse: Ladies and gentlemen, let me tell you, my economic policies are not just about big corporations or Wall Street. They're about the backbone of our nation - small businesses! You know, the mom-and-pop shops, the local cafes, the family-owned stores that make our communities unique and vibrant.\n",
      "\n",
      "Firstly, I'm going to cut through the red tape. We'll simplify regulations so that small businesses can focus on what they do best - creating jobs and serving their customers. No more endless forms or confusing rules. We'll make it easier for them to start, grow, and thrive.\n",
      "\n",
      "Secondly, I'm going to lower taxes. Not just for the wealthy, but for everyone. Small businesses will see a significant reduction in their tax burden, freeing up more money to invest in their businesses, hire more employees, and expand. We'll also provide tax incentives for businesses that create new jobs or invest in their communities.\n",
      "\n",
      "Thirdly, I'm going to invest in infrastructure. Our roads, bridges, and broadband networks are crumbling. Small businesses need reliable infrastructure to operate efficiently. By investing in infrastructure, we'll create jobs and make it easier for small businesses to reach new markets.\n",
      "\n",
      "Lastly, I'm going to support small business owners. We'll provide mentorship programs, access to capital, and resources to help them succeed. We'll also create a small business-friendly environment that encourages innovation and entrepreneurship.\n",
      "\n",
      "So, when you vote for me, you're not just voting for a candidate. You're voting for a future where small businesses can flourish, creating jobs and prosperity for all. Together, we can make America great again!\n",
      "\n",
      "id='0063d096ca724f8ca86a5cce6ca98bc3' object='chat.completion' model='ministral-8b-latest' usage=UsageInfo(prompt_tokens=613, completion_tokens=36, total_tokens=649) created=1737746291 choices=[ChatCompletionChoice(index=0, message=AssistantMessage(content='{\\n  \"confidence\": 0.95,\\n  \"aggressiveness\": 0.75,\\n  \"enthusiasm\": 0.85\\n}', tool_calls=None, prefix=False, role='assistant'), finish_reason='stop')]\n",
      "[LOG] Émotions mises à jour de Donald Trump: {'confidence': 0.95, 'aggressiveness': 0.75, 'enthusiasm': 0.85}\n",
      "\n",
      "[Kamala Harris] Réponse: Thank you for asking that question. I'm glad you're interested in understanding how my economic policies will support small businesses. My approach is twofold: fostering a supportive environment and providing targeted assistance.\n",
      "\n",
      "Firstly, I propose a reduction in regulatory burdens. Small businesses often struggle with excessive red tape, which can hinder their growth and innovation. By simplifying regulations and streamlining processes, we can help small businesses operate more efficiently and focus on what they do best—creating jobs and driving local economies.\n",
      "\n",
      "Secondly, I plan to invest in infrastructure and education. A robust infrastructure network is crucial for small businesses to reach new markets and customers. By investing in roads, broadband, and other essential infrastructure, we can lower the costs of doing business and make it easier for small businesses to thrive.\n",
      "\n",
      "Additionally, I will prioritize education and workforce development. A skilled workforce is vital for small businesses to compete and grow. By investing in vocational training and education programs, we can ensure that small businesses have access to the talent they need to succeed.\n",
      "\n",
      "Lastly, I propose targeted financial assistance programs. Small businesses often face challenges in accessing capital. By providing grants, low-interest loans, and other financial support, we can help small businesses start, expand, and innovate.\n",
      "\n",
      "In summary, my policies aim to create a supportive environment for small businesses by reducing regulatory burdens, investing in infrastructure and education, and providing targeted financial assistance. These measures will not only help small businesses grow but also contribute to sustainable economic growth and job creation.\n",
      "\n",
      "id='1b830e9e1e0c46b3805ead25ea7fc408' object='chat.completion' model='ministral-8b-latest' usage=UsageInfo(prompt_tokens=581, completion_tokens=55, total_tokens=636) created=1737746299 choices=[ChatCompletionChoice(index=0, message=AssistantMessage(content='{\\n  \"confidence\": 0.9,\\n  \"determination\": 0.95,\\n  \"calmness\": 0.85,\\n  \"empathy\": 0.7,\\n  \"engagement\": 0.8\\n}', tool_calls=None, prefix=False, role='assistant'), finish_reason='stop')]\n",
      "[LOG] Émotions mises à jour de Kamala Harris: {'confidence': 0.9, 'determination': 0.95, 'calmness': 0.85, 'empathy': 0.7, 'engagement': 0.8}\n",
      "\n",
      "\n",
      "==================================================\n",
      "Tour de débat n°2\n",
      "==================================================\n",
      "\n",
      "[MODERATOR] Question: What is your plan to reduce unemployment?\n",
      "\n",
      "[Donald Trump] Réponse: **Plan to Reduce Unemployment:**\n",
      "\n",
      "Ladies and gentlemen, let me tell you, my plan to reduce unemployment is straightforward and effective. We're going to put America back to work, and we're going to do it by focusing on job creation, workforce development, and supporting our workers.\n",
      "\n",
      "Firstly, we're going to cut taxes for businesses and individuals. Lower taxes mean more money in your pocket and more capital for businesses to invest in growth and hiring. We'll also eliminate unnecessary regulations that stifle job growth.\n",
      "\n",
      "Secondly, we're going to invest in our workforce. We'll provide vocational training and education programs to ensure that Americans have the skills they need to fill the jobs of the future. We'll also create apprenticeship programs that pair workers with employers, giving them on-the-job training and a clear path to a successful career.\n",
      "\n",
      "Thirdly, we're going to support our workers. We'll provide a strong safety net, including unemployment benefits and job retraining programs, to help those who are temporarily out of work. We'll also ensure that our workers have access to affordable healthcare and childcare, so they can focus on their careers and provide for their families.\n",
      "\n",
      "Lastly, we're going to create a business-friendly environment that encourages job growth. We'll cut red tape, lower taxes, and invest in infrastructure to make it easier for businesses to operate and expand. We'll also support small businesses, which are the backbone of our economy and create the majority of new jobs.\n",
      "\n",
      "Together, these policies will create a strong, vibrant economy that puts Americans back to work and ensures that everyone has the opportunity to succeed. We can make America great again, and we can do it by putting our people first. Thank you, and God bless America!\n",
      "\n",
      "id='55c45dad7ecc4852a296e30b29bf4f86' object='chat.completion' model='ministral-8b-latest' usage=UsageInfo(prompt_tokens=1004, completion_tokens=34, total_tokens=1038) created=1737746305 choices=[ChatCompletionChoice(index=0, message=AssistantMessage(content='{\\n  \"confidence\": 0.98,\\n  \"aggressiveness\": 0.8,\\n  \"enthusiasm\": 0.9\\n}', tool_calls=None, prefix=False, role='assistant'), finish_reason='stop')]\n",
      "[LOG] Émotions mises à jour de Donald Trump: {'confidence': 0.98, 'aggressiveness': 0.8, 'enthusiasm': 0.9}\n",
      "\n"
     ]
    },
    {
     "ename": "SDKError",
     "evalue": "API error occurred: Status 429\n{\"message\":\"Requests rate limit exceeded\"}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSDKError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 307\u001b[0m\n\u001b[1;32m    304\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Pause pour simuler un timing\u001b[39;00m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;66;03m# Harris répond\u001b[39;00m\n\u001b[0;32m--> 307\u001b[0m harris_response \u001b[38;5;241m=\u001b[39m \u001b[43mharris_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrespond\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopponent_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mharris_agent\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] Réponse: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mharris_response\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    309\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 154\u001b[0m, in \u001b[0;36mAIAgent.respond\u001b[0;34m(self, input_text, opponent_state)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;124;03mGénère une réponse basée sur le contexte fourni.\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;124;03m:return: Réponse de l'IA\u001b[39;00m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconversation_history\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: input_text})\n\u001b[0;32m--> 154\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_response\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[43minstructions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mopponent_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mopponent_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m    \u001b[49m\u001b[43menvironment_description\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mN/A\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    158\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconversation_history\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname: response})\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "Cell \u001b[0;32mIn[6], line 200\u001b[0m, in \u001b[0;36mAIAgent._generate_response\u001b[0;34m(self, instructions, opponent_state, environment_description, max_tokens)\u001b[0m\n\u001b[1;32m    177\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    178\u001b[0m     {\n\u001b[1;32m    179\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    195\u001b[0m     },\n\u001b[1;32m    196\u001b[0m ]\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# Hypothetical call to LLM API\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m chat_response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomplete\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMODEL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_tokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m    \u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;66;03m# ------------------------------------------------\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m chat_response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\n",
      "File \u001b[0;32m~/Documents/hackaton_mistral/.venv/lib/python3.13/site-packages/mistralai/chat.py:153\u001b[0m, in \u001b[0;36mChat.complete\u001b[0;34m(self, model, messages, temperature, top_p, max_tokens, stream, stop, random_seed, response_format, tools, tool_choice, presence_penalty, frequency_penalty, n, prediction, safe_prompt, retries, server_url, timeout_ms, http_headers)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m utils\u001b[38;5;241m.\u001b[39mmatch_response(http_res, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m4XX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    152\u001b[0m     http_res_text \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mstream_to_text(http_res)\n\u001b[0;32m--> 153\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m models\u001b[38;5;241m.\u001b[39mSDKError(\n\u001b[1;32m    154\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAPI error occurred\u001b[39m\u001b[38;5;124m\"\u001b[39m, http_res\u001b[38;5;241m.\u001b[39mstatus_code, http_res_text, http_res\n\u001b[1;32m    155\u001b[0m     )\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m utils\u001b[38;5;241m.\u001b[39mmatch_response(http_res, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m5XX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    157\u001b[0m     http_res_text \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mstream_to_text(http_res)\n",
      "\u001b[0;31mSDKError\u001b[0m: API error occurred: Status 429\n{\"message\":\"Requests rate limit exceeded\"}"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import time\n",
    "from mistralai import Mistral\n",
    "\n",
    "MISTRAL_KEY = \"MaFKaY7R8TYrOLd56ZDRBcUfatOrebRh\"\n",
    "\n",
    "MODEL = \"ministral-8b-latest\"\n",
    "\n",
    "def link_with_newline_middle(strings):\n",
    "    if len(strings) <= 1:\n",
    "        return ''.join(strings)  # Return as is for single or empty lists\n",
    "    middle = len(strings) // 2\n",
    "    # Join the first part, add '\\n', and then join the rest\n",
    "    return '\\n'.join([' '.join(strings[:middle]), ' '.join(strings[middle:])])\n",
    "\n",
    "class LLMArbitraryAgent:\n",
    "    \"\"\"\n",
    "    This class uses an LLM to update emotions by analyzing the entire context:\n",
    "    - Character traits\n",
    "    - Current/initial emotions\n",
    "    - General conversation context\n",
    "    - Conversation history\n",
    "\n",
    "    The LLM is prompted to return new emotion values in JSON format.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model=MODEL):\n",
    "        self.model = model\n",
    "\n",
    "    def update_emotions(\n",
    "        self, \n",
    "        character, \n",
    "        current_emotions, \n",
    "        general_context, \n",
    "        conversation_history\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Calls an LLM to analyze the agent's overall context\n",
    "        and produce updated emotion values in JSON format.\n",
    "\n",
    "        :param character: dict describing the AI's personality traits\n",
    "        :param current_emotions: dict of current emotion levels\n",
    "        :param general_context: dict describing the debate/topic context\n",
    "        :param conversation_history: list of message dicts representing the conversation so far\n",
    "        :return: dict of updated emotions (floats in [0.0, 1.0])\n",
    "        \"\"\"\n",
    "\n",
    "        # Prepare the prompt for the LLM\n",
    "        system_prompt = (\n",
    "            \"You are an emotion update engine. \"\n",
    "            \"Given the AI's character traits, current emotions, general context, and conversation history, \"\n",
    "            \"you will propose updated emotion values. The output must be strictly valid JSON, \"\n",
    "            \"with keys as emotion names and values as floats between 0.0 and 1.0.\\n\"\n",
    "        )\n",
    "\n",
    "        user_prompt = f\"\"\"Character: {character}\n",
    "Current Emotions: {current_emotions}\n",
    "General Context: {general_context}\n",
    "Conversation History: {conversation_history}\n",
    "\n",
    "Instructions:\n",
    "1. Analyze the above information.\n",
    "2. Propose new emotion values in valid JSON format, for example:\n",
    "   {{\n",
    "     \"confidence\": 0.9,\n",
    "     \"aggressiveness\": 0.7,\n",
    "     \"enthusiasm\": 0.8\n",
    "   }}\n",
    "Make sure each emotion is within [0.0, 1.0].\n",
    "\"\"\"\n",
    "\n",
    "        # Prepare messages for the chat completion API\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "\n",
    "        # ------------------------------\n",
    "        # Hypothetical call to LLM API\n",
    "        # ------------------------------\n",
    "        chat_response = client.chat.complete(\n",
    "            model=self.model,\n",
    "            messages=messages,\n",
    "            response_format={\"type\":\"json_object\"},\n",
    "            max_tokens=200  # Ajustez si nécessaire\n",
    "        )\n",
    "\n",
    "        print(chat_response)\n",
    "        # ------------------------------------------------\n",
    "\n",
    "        # Extract LLM's raw text response (which should be JSON)\n",
    "        llm_output_text = chat_response.choices[0].message.content.strip()\n",
    "        # Remove any ```json or ``` markers\n",
    "        cleaned_response = re.sub(r\"```(json)?\", \"\", llm_output_text).strip()\n",
    "\n",
    "        # Parse JSON\n",
    "        try:\n",
    "            updated_emotions = json.loads(cleaned_response)\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Error: LLM response could not be parsed as JSON. Response: {cleaned_response}\")\n",
    "            updated_emotions = current_emotions  # fallback\n",
    "\n",
    "        # Clamp to [0.0, 1.0]\n",
    "        final_emotions = {}\n",
    "        for emotion, val in updated_emotions.items():\n",
    "            if isinstance(val, (int, float)):\n",
    "                clamped_val = max(0.0, min(1.0, float(val)))\n",
    "                final_emotions[emotion] = clamped_val\n",
    "            else:\n",
    "                final_emotions[emotion] = current_emotions.get(emotion, 0.5)\n",
    "\n",
    "        return final_emotions\n",
    "\n",
    "\n",
    "class AIAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        name,\n",
    "        character,\n",
    "        emotions,\n",
    "        goal,\n",
    "        general_context,\n",
    "        arbitrary_agent=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialise l'agent IA avec ses attributs de base.\n",
    "\n",
    "        :param name: Nom de l'agent\n",
    "        :param character: dict décrivant la personnalité de l'IA\n",
    "        :param emotions: dict des émotions actuelles de l'IA\n",
    "        :param goal: Objectif principal de l'IA\n",
    "        :param general_context: Contexte général (ex: sujet du débat)\n",
    "        :param arbitrary_agent: Objet gérant la logique de mise à jour des émotions\n",
    "        \"\"\"\n",
    "        self.name = name\n",
    "        self.character = character\n",
    "        self.emotions = emotions\n",
    "        self.goal = goal\n",
    "        self.general_context = general_context\n",
    "        self.arbitrary_agent = arbitrary_agent\n",
    "        self.conversation_history = []\n",
    "\n",
    "    def respond(self, input_text, opponent_state):\n",
    "        \"\"\"\n",
    "        Génère une réponse basée sur le contexte fourni.\n",
    "\n",
    "        :param input_text: Texte reçu\n",
    "        :param opponent_state: État actuel de l'adversaire (dict)\n",
    "        :return: Réponse de l'IA\n",
    "        \"\"\"\n",
    "        self.conversation_history.append({\"user\": input_text})\n",
    "        \n",
    "        response = self._generate_response(\n",
    "            instructions=input_text,\n",
    "            opponent_state=opponent_state,\n",
    "            environment_description=\"N/A\"\n",
    "        )\n",
    "        self.conversation_history.append({self.name: response})\n",
    "        return response\n",
    "\n",
    "    def update_emotions(self):\n",
    "        \"\"\"\n",
    "        Met à jour les émotions en fonction d'une analyse via l'arbitrary_agent (LLM).\n",
    "        \"\"\"\n",
    "        if self.arbitrary_agent is not None:\n",
    "            self.emotions = self.arbitrary_agent.update_emotions(\n",
    "                character=self.character,\n",
    "                current_emotions=self.emotions,\n",
    "                general_context=self.general_context,\n",
    "                conversation_history=self.conversation_history\n",
    "            )\n",
    "        else:\n",
    "            print(\"No arbitrary agent provided. Emotions remain unchanged.\")\n",
    "\n",
    "    def _generate_response(self, instructions, opponent_state, environment_description, max_tokens=None):\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": (\n",
    "                    f\"General context: {self.general_context}\\n\"\n",
    "                    f\"Character: {self.character}\\n\"\n",
    "                    f\"Goal: {self.goal}\\n\"\n",
    "                    f\"Emotions: {self.emotions}\\n\"\n",
    "                    f\"Environment: {environment_description}\\n\"\n",
    "                ),\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": (\n",
    "                    f\"Instructions: {instructions}\\n\"\n",
    "                    f\"Opponent state: {opponent_state}\\n\"\n",
    "                    f\"Conversation history: {self.conversation_history}\"\n",
    "                ),\n",
    "            },\n",
    "        ]\n",
    "        # ------------------------------\n",
    "        # Hypothetical call to LLM API\n",
    "        # ------------------------------\n",
    "        chat_response = client.chat.complete(\n",
    "            model=MODEL,\n",
    "            messages=messages,\n",
    "            max_tokens=max_tokens if max_tokens else None,\n",
    "            \n",
    "        )\n",
    "        # ------------------------------------------------\n",
    "        return chat_response.choices[0].message.content\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Configuration générale du contexte et des agents\n",
    "# ------------------------------------------------\n",
    "\n",
    "general_context = {\n",
    "    \"topic\": \"Economic Policy\",\n",
    "    \"setting\": \"Televised national debate\",\n",
    "    \"audience\": \"Undecided voters seeking clarity on candidates' economic plans\"\n",
    "}\n",
    "\n",
    "# Donald Trump\n",
    "trump_character = {\n",
    "    \"dominant\": True,\n",
    "    \"charismatic\": True,\n",
    "    \"unpredictable\": True,\n",
    "    \"assertive\": True,\n",
    "    \"narcissistic\": True\n",
    "}\n",
    "trump_emotions = {\n",
    "    \"confidence\": 0.9,\n",
    "    \"aggressiveness\": 0.8,\n",
    "    \"enthusiasm\": 0.7\n",
    "}\n",
    "trump_goal = \"Persuade the audience that his economic policies will lead to national prosperity and job growth.\"\n",
    "\n",
    "# Kamala Harris\n",
    "harris_character = {\n",
    "    \"analytical\": True,\n",
    "    \"empathetic\": True,\n",
    "    \"methodical\": True,\n",
    "    \"decisive\": True,\n",
    "    \"direct\": True\n",
    "}\n",
    "harris_emotions = {\n",
    "    \"confidence\": 0.85,\n",
    "    \"determination\": 0.9,\n",
    "    \"calmness\": 0.8\n",
    "}\n",
    "harris_goal = \"Convince the audience that her economic plans offer sustainable growth and address income inequality.\"\n",
    "\n",
    "# Agent pour la mise à jour des émotions via LLM\n",
    "llm_arbitrary_agent = LLMArbitraryAgent(model=MODEL)\n",
    "\n",
    "# Création des deux agents\n",
    "trump_agent = AIAgent(\n",
    "    name=\"Donald Trump\",\n",
    "    character=trump_character,\n",
    "    emotions=trump_emotions,\n",
    "    goal=trump_goal,\n",
    "    general_context=general_context,\n",
    "    arbitrary_agent=llm_arbitrary_agent\n",
    ")\n",
    "\n",
    "harris_agent = AIAgent(\n",
    "    name=\"Kamala Harris\",\n",
    "    character=harris_character,\n",
    "    emotions=harris_emotions,\n",
    "    goal=harris_goal,\n",
    "    general_context=general_context,\n",
    "    arbitrary_agent=llm_arbitrary_agent\n",
    ")\n",
    "\n",
    "\n",
    "# Initialize the Mistral client\n",
    "client = Mistral(api_key=MISTRAL_KEY)\n",
    "# ---------------------------------------\n",
    "# Boucle pour générer le dialogue\n",
    "# ---------------------------------------\n",
    "# Exemples de questions (vous pouvez en ajouter ou utiliser des inputs directs)\n",
    "questions = [\n",
    "    \"How will your policies affect small businesses?\",\n",
    "    \"What is your plan to reduce unemployment?\",\n",
    "    \"How do you intend to tackle national debt?\",\n",
    "]\n",
    "\n",
    "NUMBER_OF_TURNS = 3  # Nombre de tours de dialogue\n",
    "\n",
    "for turn in range(NUMBER_OF_TURNS):\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"Tour de débat n°{turn+1}\")\n",
    "    print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "    # Sélection d'une question (simplement en rotation ici)\n",
    "    question = questions[turn % len(questions)]\n",
    "    print(f\"[MODERATOR] Question: {question}\\n\")\n",
    "\n",
    "    # Trump répond\n",
    "    trump_response = trump_agent.respond(input_text=question, opponent_state={})\n",
    "    print(f\"[{trump_agent.name}] Réponse: {trump_response}\\n\")\n",
    "    time.sleep(1) \n",
    "\n",
    "    # Mise à jour des émotions de Trump\n",
    "    trump_agent.update_emotions()\n",
    "    print(f\"[LOG] Émotions mises à jour de {trump_agent.name}: {trump_agent.emotions}\\n\")\n",
    "    time.sleep(1)  # Pause pour simuler un timing\n",
    "\n",
    "    # Harris répond\n",
    "    harris_response = harris_agent.respond(input_text=question, opponent_state={})\n",
    "    print(f\"[{harris_agent.name}] Réponse: {harris_response}\\n\")\n",
    "    time.sleep(1)\n",
    "\n",
    "    # Mise à jour des émotions de Harris\n",
    "    harris_agent.update_emotions()\n",
    "    print(f\"[LOG] Émotions mises à jour de {harris_agent.name}: {harris_agent.emotions}\\n\")\n",
    "    time.sleep(1)\n",
    "\n",
    "# Fin de la boucle de débat\n",
    "print(\"=== Fin du débat ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debate saved to debate_log.txt\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import time\n",
    "from mistralai import Mistral\n",
    "\n",
    "MISTRAL_KEY = \"MaFKaY7R8TYrOLd56ZDRBcUfatOrebRh\"\n",
    "\n",
    "MODEL = \"ministral-8b-latest\"\n",
    "\n",
    "def link_with_newline_middle(strings):\n",
    "    if len(strings) <= 1:\n",
    "        return ''.join(strings)  # Return as is for single or empty lists\n",
    "    middle = len(strings) // 2\n",
    "    # Join the first part, add '\\n', and then join the rest\n",
    "    return '\\n'.join([' '.join(strings[:middle]), ' '.join(strings[middle:])])\n",
    "\n",
    "class LLMArbitraryAgent:\n",
    "    \"\"\"\n",
    "    Cette classe utilise un LLM pour différentes tâches, notamment :\n",
    "    - Mise à jour des émotions\n",
    "    - Génération d'un contexte mémoire (\"memory context\") à partir de l'historique et du caractère\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model=MODEL):\n",
    "        self.model = model\n",
    "\n",
    "    def update_emotions(\n",
    "        self, \n",
    "        character, \n",
    "        current_emotions, \n",
    "        general_context, \n",
    "        conversation_history\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Analyse le contexte global et produit de nouvelles valeurs d'émotions en JSON.\n",
    "        \"\"\"\n",
    "\n",
    "        system_prompt = (\n",
    "            \"You are an emotion update engine. \"\n",
    "            \"Given the AI's character traits, current emotions, general context, and conversation history, \"\n",
    "            \"you will propose updated emotion values. The output must be strictly valid JSON, \"\n",
    "            \"with keys as emotion names and values as floats between 0.0 and 1.0.\\n\"\n",
    "        )\n",
    "\n",
    "        user_prompt = f\"\"\"Character: {character}\n",
    "Current Emotions: {current_emotions}\n",
    "General Context: {general_context}\n",
    "Conversation History: {conversation_history}\n",
    "\n",
    "Instructions:\n",
    "1. Analyze the above information.\n",
    "2. Propose new emotion values in valid JSON format, for example:\n",
    "   {{\n",
    "     \"confidence\": 0.9,\n",
    "     \"aggressiveness\": 0.7,\n",
    "     \"enthusiasm\": 0.8\n",
    "   }}\n",
    "Make sure each emotion is within [0.0, 1.0].\n",
    "\"\"\"\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "\n",
    "        # ------------------------------\n",
    "        # Appel hypothétique à l'API LLM\n",
    "        # ------------------------------\n",
    "        chat_response = client.chat.complete(\n",
    "            model=self.model,\n",
    "            messages=messages,\n",
    "            response_format={\"type\":\"json_object\"},\n",
    "            max_tokens=200\n",
    "        )\n",
    "        # ------------------------------------------------\n",
    "\n",
    "        llm_output_text = chat_response.choices[0].message.content.strip()\n",
    "        cleaned_response = re.sub(r\"```(json)?\", \"\", llm_output_text).strip()\n",
    "\n",
    "        try:\n",
    "            updated_emotions = json.loads(cleaned_response)\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Error: LLM response could not be parsed as JSON. Response: {cleaned_response}\")\n",
    "            updated_emotions = current_emotions  # fallback\n",
    "\n",
    "        # Clamp des valeurs d'émotion à [0.0, 1.0]\n",
    "        final_emotions = {}\n",
    "        for emotion, val in updated_emotions.items():\n",
    "            if isinstance(val, (int, float)):\n",
    "                clamped_val = max(0.0, min(1.0, float(val)))\n",
    "                final_emotions[emotion] = clamped_val\n",
    "            else:\n",
    "                # si le LLM renvoie autre chose qu'un nombre\n",
    "                final_emotions[emotion] = current_emotions.get(emotion, 0.5)\n",
    "\n",
    "        return final_emotions\n",
    "\n",
    "    # --- NEW/UPDATED ---\n",
    "    def create_memory_context(\n",
    "        self,\n",
    "        character,\n",
    "        emotions,\n",
    "        conversation_history,\n",
    "        additional_instructions=\"Summarize recent key points and emotional undertones.\"\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Crée un contexte mémoire (memory context) à partir de l'historique de conversation,\n",
    "        du caractère et des émotions actuelles de l'agent, via un LLM.\n",
    "        \n",
    "        :param character: dict décrivant la personnalité de l'IA\n",
    "        :param emotions: dict d'émotions courantes\n",
    "        :param conversation_history: liste d'échanges dans la conversation\n",
    "        :param additional_instructions: instructions supplémentaires pour affiner le résumé\n",
    "        :return: un dict (ou texte) représentant le nouveau contexte mémoire\n",
    "        \"\"\"\n",
    "\n",
    "        system_prompt = (\n",
    "            \"You are a memory transformation engine. \"\n",
    "            \"Based on the agent's conversation history, character, and current emotions, \"\n",
    "            \"produce a concise memory context (in valid JSON) that highlights important points, \"\n",
    "            \"relationship dynamics, and relevant emotional undercurrents. \"\n",
    "            \"This memory context will help the agent craft its future responses.\\n\"\n",
    "        )\n",
    "\n",
    "        # On peut construire un prompt utilisateur plus détaillé si nécessaire\n",
    "        user_prompt = f\"\"\"\n",
    "Character: {character}\n",
    "Emotions: {emotions}\n",
    "Conversation History: {conversation_history}\n",
    "\n",
    "Task:\n",
    "- {additional_instructions}\n",
    "- Return a structured memory context in valid JSON. Example:\n",
    "  {{\n",
    "    \"summary\": \"Brief summary of important points...\",\n",
    "    \"emotionalTone\": \"general emotional tone...\"\n",
    "  }}\n",
    "\"\"\"\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "\n",
    "        # ------------------------------\n",
    "        # Appel hypothétique à l'API LLM\n",
    "        # ------------------------------\n",
    "        chat_response = client.chat.complete(\n",
    "            model=self.model,\n",
    "            messages=messages,\n",
    "            response_format={\"type\":\"json_object\"},\n",
    "            max_tokens=300  # Ajustez selon vos besoins\n",
    "        )\n",
    "        # ------------------------------------------------\n",
    "\n",
    "        llm_output_text = chat_response.choices[0].message.content.strip()\n",
    "        cleaned_response = re.sub(r\"```(json)?\", \"\", llm_output_text).strip()\n",
    "\n",
    "        try:\n",
    "            memory_context = json.loads(cleaned_response)\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Error: LLM response could not be parsed as JSON for memory context. Response: {cleaned_response}\")\n",
    "            # On renvoie un contexte par défaut en cas d'échec\n",
    "            memory_context = {\n",
    "                \"summary\": \"No valid summary\",\n",
    "                \"emotionalTone\": \"neutral\"\n",
    "            }\n",
    "\n",
    "        return memory_context\n",
    "\n",
    "\n",
    "class AIAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        name,\n",
    "        character,\n",
    "        emotions,\n",
    "        goal,\n",
    "        general_context,\n",
    "        arbitrary_agent=None\n",
    "    ):\n",
    "        self.name = name\n",
    "        self.character = character\n",
    "        self.emotions = emotions\n",
    "        self.goal = goal\n",
    "        self.general_context = general_context\n",
    "        self.arbitrary_agent = arbitrary_agent\n",
    "        self.conversation_history = []\n",
    "\n",
    "        # --- NEW/UPDATED ---\n",
    "        # Initialise un champ de mémoire (qui sera mis à jour par create_memory_context)\n",
    "        self.memory_context = {}\n",
    "\n",
    "    def respond(self, input_text, opponent_state):\n",
    "        \"\"\"\n",
    "        Génère une réponse basée sur le contexte fourni.\n",
    "        \"\"\"\n",
    "        self.conversation_history.append({\"user\": input_text})\n",
    "        \n",
    "        response = self._generate_response(\n",
    "            instructions=input_text,\n",
    "            opponent_state=opponent_state,\n",
    "            environment_description=\"N/A\"\n",
    "        )\n",
    "        self.conversation_history.append({self.name: response})\n",
    "        return response\n",
    "\n",
    "    def update_emotions(self):\n",
    "        \"\"\"\n",
    "        Met à jour les émotions en fonction d'une analyse via l'arbitrary_agent (LLM).\n",
    "        \"\"\"\n",
    "        if self.arbitrary_agent is not None:\n",
    "            self.emotions = self.arbitrary_agent.update_emotions(\n",
    "                character=self.character,\n",
    "                current_emotions=self.emotions,\n",
    "                general_context=self.general_context,\n",
    "                conversation_history=self.conversation_history\n",
    "            )\n",
    "        else:\n",
    "            print(\"No arbitrary agent provided. Emotions remain unchanged.\")\n",
    "\n",
    "    # --- NEW/UPDATED ---\n",
    "    def update_memory_context(self):\n",
    "        \"\"\"\n",
    "        Met à jour le contexte mémoire (memory context) à partir de l'historique,\n",
    "        des émotions et du caractère via l'arbitrary_agent.\n",
    "        \"\"\"\n",
    "        if self.arbitrary_agent is not None:\n",
    "            self.memory_context = self.arbitrary_agent.create_memory_context(\n",
    "                character=self.character,\n",
    "                emotions=self.emotions,\n",
    "                conversation_history=self.conversation_history\n",
    "            )\n",
    "        else:\n",
    "            print(\"No arbitrary agent provided. Memory context remains unchanged.\")\n",
    "\n",
    "    def _generate_response(self, instructions, opponent_state, environment_description, max_tokens=None):\n",
    "        # --- NEW/UPDATED ---\n",
    "        # On ajoute self.memory_context dans le système de prompt pour que l'LLM en tienne compte\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": (\n",
    "                    f\"General context: {self.general_context}\\n\"\n",
    "                    f\"Character: {self.character}\\n\"\n",
    "                    f\"Goal: {self.goal}\\n\"\n",
    "                    f\"Emotions: {self.emotions}\\n\"\n",
    "                    f\"Memory Context: {self.memory_context}\\n\"  # <-- Ajout important\n",
    "                    f\"Environment: {environment_description}\\n\"\n",
    "                ),\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": (\n",
    "                    f\"Instructions: {instructions}\\n\"\n",
    "                    f\"Opponent state: {opponent_state}\\n\"\n",
    "                    f\"Conversation history: {self.conversation_history}\"\n",
    "                ),\n",
    "            },\n",
    "        ]\n",
    "        # ------------------------------\n",
    "        # Appel hypothétique à l'API LLM\n",
    "        # ------------------------------\n",
    "        chat_response = client.chat.complete(\n",
    "            model=MODEL,\n",
    "            messages=messages,\n",
    "            max_tokens=max_tokens if max_tokens else None,\n",
    "        )\n",
    "        # ------------------------------------------------\n",
    "\n",
    "        return chat_response.choices[0].message.content\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Configuration générale du contexte et des agents\n",
    "# ------------------------------------------------\n",
    "\n",
    "general_context = {\n",
    "    \"topic\": \"Economic Policy\",\n",
    "    \"setting\": \"Televised national debate\",\n",
    "    \"audience\": \"Undecided voters seeking clarity on candidates' economic plans\"\n",
    "}\n",
    "\n",
    "# Donald Trump\n",
    "trump_character = {\n",
    "    \"dominant\": True,\n",
    "    \"charismatic\": True,\n",
    "    \"unpredictable\": True,\n",
    "    \"assertive\": True,\n",
    "    \"narcissistic\": True\n",
    "}\n",
    "trump_emotions = {\n",
    "    \"confidence\": 0.9,\n",
    "    \"aggressiveness\": 0.8,\n",
    "    \"enthusiasm\": 0.7\n",
    "}\n",
    "trump_goal = \"Persuade the audience that his economic policies will lead to national prosperity and job growth.\"\n",
    "\n",
    "# Kamala Harris\n",
    "harris_character = {\n",
    "    \"analytical\": True,\n",
    "    \"empathetic\": True,\n",
    "    \"methodical\": True,\n",
    "    \"decisive\": True,\n",
    "    \"direct\": True\n",
    "}\n",
    "harris_emotions = {\n",
    "    \"confidence\": 0.85,\n",
    "    \"determination\": 0.9,\n",
    "    \"calmness\": 0.8\n",
    "}\n",
    "harris_goal = \"Convince the audience that her economic plans offer sustainable growth and address income inequality.\"\n",
    "\n",
    "# Agent pour la mise à jour des émotions et création de mémoire via LLM\n",
    "llm_arbitrary_agent = LLMArbitraryAgent(model=MODEL)\n",
    "\n",
    "# Création des deux agents\n",
    "trump_agent = AIAgent(\n",
    "    name=\"Donald Trump\",\n",
    "    character=trump_character,\n",
    "    emotions=trump_emotions,\n",
    "    goal=trump_goal,\n",
    "    general_context=general_context,\n",
    "    arbitrary_agent=llm_arbitrary_agent\n",
    ")\n",
    "\n",
    "harris_agent = AIAgent(\n",
    "    name=\"Kamala Harris\",\n",
    "    character=harris_character,\n",
    "    emotions=harris_emotions,\n",
    "    goal=harris_goal,\n",
    "    general_context=general_context,\n",
    "    arbitrary_agent=llm_arbitrary_agent\n",
    ")\n",
    "\n",
    "\n",
    "# Initialisation du client Mistral\n",
    "client = Mistral(api_key=MISTRAL_KEY)\n",
    "\n",
    "# ---------------------------------------\n",
    "# Boucle simple pour générer le dialogue\n",
    "# ---------------------------------------\n",
    "questions = [\n",
    "    \"How will your policies affect small businesses?\",\n",
    "    \"What is your plan to reduce unemployment?\",\n",
    "    \"How do you intend to tackle national debt?\",\n",
    "]\n",
    "\n",
    "NUMBER_OF_TURNS = 3\n",
    "\n",
    "# Function to save the debate to a .txt file\n",
    "def save_debate_to_file(filename, debate_log):\n",
    "    \"\"\"\n",
    "    Saves the debate log to a .txt file.\n",
    "\n",
    "    :param filename: The name of the file to save the debate log.\n",
    "    :param debate_log: A list of strings representing the debate log.\n",
    "    \"\"\"\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
    "        for entry in debate_log:\n",
    "            file.write(entry + \"\\n\")\n",
    "    print(f\"Debate saved to {filename}\")\n",
    "\n",
    "# Initialize a log for the debate\n",
    "debate_log = []\n",
    "\n",
    "# Start the debate loop\n",
    "for turn in range(NUMBER_OF_TURNS):\n",
    "    log_entry = \"\\n\" + \"=\" * 50\n",
    "    log_entry += f\"\\nTour de débat n°{turn+1}\\n\" + \"=\" * 50 + \"\\n\"\n",
    "    debate_log.append(log_entry)\n",
    "    time.sleep(1.5)\n",
    "\n",
    "    # Sélection d'une question\n",
    "    question = questions[turn % len(questions)]\n",
    "    log_entry = f\"[MODERATOR] Question: {question}\\n\"\n",
    "    debate_log.append(log_entry)\n",
    "    time.sleep(1.5)\n",
    "\n",
    "    # Trump répond\n",
    "    trump_response = trump_agent.respond(input_text=question, opponent_state={})\n",
    "    log_entry = f\"[{trump_agent.name}] Réponse: {trump_response}\\n\"\n",
    "    debate_log.append(log_entry)\n",
    "    time.sleep(1.5)\n",
    "\n",
    "    # Mise à jour des émotions de Trump\n",
    "    trump_agent.update_emotions()\n",
    "    log_entry = f\"[LOG] Émotions mises à jour de {trump_agent.name}: {trump_agent.emotions}\\n\"\n",
    "    debate_log.append(log_entry)\n",
    "    time.sleep(1.5)\n",
    "\n",
    "    # Mise à jour du contexte mémoire de Trump\n",
    "    trump_agent.update_memory_context()\n",
    "    log_entry = f\"[LOG] Contexte mémoire de {trump_agent.name}:\\n{trump_agent.memory_context}\\n\"\n",
    "    debate_log.append(log_entry)\n",
    "    time.sleep(1.5)\n",
    "\n",
    "    # Harris répond\n",
    "    harris_response = harris_agent.respond(input_text=question, opponent_state={})\n",
    "    log_entry = f\"[{harris_agent.name}] Réponse: {harris_response}\\n\"\n",
    "    debate_log.append(log_entry)\n",
    "    time.sleep(1.5)\n",
    "\n",
    "    # Mise à jour des émotions de Harris\n",
    "    harris_agent.update_emotions()\n",
    "    log_entry = f\"[LOG] Émotions mises à jour de {harris_agent.name}: {harris_agent.emotions}\\n\"\n",
    "    debate_log.append(log_entry)\n",
    "    time.sleep(1.5)\n",
    "\n",
    "    # Mise à jour du contexte mémoire de Harris\n",
    "    harris_agent.update_memory_context()\n",
    "    log_entry = f\"[LOG] Contexte mémoire de {harris_agent.name}:\\n{harris_agent.memory_context}\\n\"\n",
    "    debate_log.append(log_entry)\n",
    "    time.sleep(1.5)\n",
    "\n",
    "# Fin du débat\n",
    "debate_log.append(\"=== Fin du débat ===\")\n",
    "\n",
    "# Save the debate log to a file\n",
    "save_debate_to_file(\"debate_log_fun_0.txt\", debate_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ladies and Gentlemen, thank you for having me here today. I stand before you not just as a candidate, but as a visionary who believes in the power of our nation's potential. We are at a crossroads, and I am here to tell you that my economic policies are the key to unlocking a future of prosperity and job growth for every American.\n"
     ]
    }
   ],
   "source": [
    "print(trump_agent._generate_response(\"first person to take the parole, have a concise response\", \"waiting\", \"people in the crowd are waiting\"))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debate saved to debate_log_fun_0.txt\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import time\n",
    "from mistralai import Mistral\n",
    "\n",
    "MISTRAL_KEY = \"MaFKaY7R8TYrOLd56ZDRBcUfatOrebRh\"\n",
    "MODEL = \"ministral-8b-latest\"\n",
    "\n",
    "def link_with_newline_middle(strings):\n",
    "    if len(strings) <= 1:\n",
    "        return ''.join(strings)  # Return as is for single or empty lists\n",
    "    middle = len(strings) // 2\n",
    "    # Join the first part, add '\\n', and then join the rest\n",
    "    return '\\n'.join([' '.join(strings[:middle]), ' '.join(strings[middle:])])\n",
    "\n",
    "# ---------------------------------------\n",
    "# LLMArbitraryAgent class\n",
    "# ---------------------------------------\n",
    "class LLMArbitraryAgent:\n",
    "    \"\"\"\n",
    "    Cette classe utilise un LLM pour différentes tâches, notamment :\n",
    "    - Mise à jour des émotions (update_emotions)\n",
    "    - Mise à jour des attitudes (update_attitude)\n",
    "    - Génération d'un contexte mémoire (create_memory_context)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model=MODEL):\n",
    "        self.model = model\n",
    "\n",
    "    def update_emotions(\n",
    "        self, \n",
    "        character, \n",
    "        current_emotions, \n",
    "        general_context, \n",
    "        conversation_history\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Analyse le contexte global et produit de nouvelles valeurs d'émotions en JSON.\n",
    "        \"\"\"\n",
    "\n",
    "        system_prompt = (\n",
    "            \"You are an emotion update engine. \"\n",
    "            \"Given the AI's character traits, current emotions, general context, and conversation history, \"\n",
    "            \"you will propose updated emotion values. The output must be strictly valid JSON, \"\n",
    "            \"with keys as emotion names and values as floats between 0.0 and 1.0.\\n\"\n",
    "        )\n",
    "\n",
    "        user_prompt = f\"\"\"Character: {character}\n",
    "Current Emotions: {current_emotions}\n",
    "General Context: {general_context}\n",
    "Conversation History: {conversation_history}\n",
    "\n",
    "Instructions:\n",
    "1. Analyze the above information.\n",
    "2. Propose new emotion values in valid JSON format, for example:\n",
    "   {{\n",
    "     \"confidence\": 0.9,\n",
    "     \"aggressiveness\": 0.7,\n",
    "     \"enthusiasm\": 0.8\n",
    "   }}\n",
    "Make sure each emotion is within [0.0, 1.0].\n",
    "\"\"\"\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "\n",
    "        # ------------------------------\n",
    "        # Appel hypothétique à l'API LLM\n",
    "        # ------------------------------\n",
    "        client = Mistral(api_key=MISTRAL_KEY)\n",
    "        chat_response = client.chat.complete(\n",
    "            model=self.model,\n",
    "            messages=messages,\n",
    "            response_format={\"type\":\"json_object\"},\n",
    "            max_tokens=200\n",
    "        )\n",
    "        # ------------------------------------------------\n",
    "\n",
    "        llm_output_text = chat_response.choices[0].message.content.strip()\n",
    "        cleaned_response = re.sub(r\"```(json)?\", \"\", llm_output_text).strip()\n",
    "\n",
    "        try:\n",
    "            updated_emotions = json.loads(cleaned_response)\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Error: LLM response could not be parsed as JSON. Response: {cleaned_response}\")\n",
    "            updated_emotions = current_emotions  # fallback\n",
    "\n",
    "        # Clamp des valeurs d'émotion à [0.0, 1.0]\n",
    "        final_emotions = {}\n",
    "        for emotion, val in updated_emotions.items():\n",
    "            if isinstance(val, (int, float)):\n",
    "                clamped_val = max(0.0, min(1.0, float(val)))\n",
    "                final_emotions[emotion] = clamped_val\n",
    "            else:\n",
    "                final_emotions[emotion] = current_emotions.get(emotion, 0.5)\n",
    "\n",
    "        return final_emotions\n",
    "\n",
    "    # NEW: Update Attitude\n",
    "    def update_attitude(\n",
    "        self,\n",
    "        character,\n",
    "        current_attitude,\n",
    "        general_context,\n",
    "        conversation_history\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Analyse le contexte global et propose une mise à jour de l'attitude en JSON.\n",
    "        Les attitudes décrivent le comportement général de l'agent (ex: aggressiveness, politeness, etc.).\n",
    "        Output must be strictly valid JSON with string or float values (depending on your design).\n",
    "        \"\"\"\n",
    "\n",
    "        system_prompt = (\n",
    "            \"You are an attitude update engine. \"\n",
    "            \"Given the AI's character traits, current attitude, general context, and conversation history, \"\n",
    "            \"you will propose updated attitude traits. The output must be strictly valid JSON.\\n\"\n",
    "        )\n",
    "\n",
    "        user_prompt = f\"\"\"Character: {character}\n",
    "Current Attitude: {current_attitude}\n",
    "General Context: {general_context}\n",
    "Conversation History: {conversation_history}\n",
    "\n",
    "Instructions:\n",
    "1. Analyze the above info and update the attitude description.\n",
    "2. Return valid JSON. Example:\n",
    "   {{\n",
    "     \"tone\": \"aggressive\",\n",
    "     \"behavior\": \"insulting\",\n",
    "     \"politeness\": 0.2\n",
    "   }}\n",
    "\"\"\"\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "\n",
    "        # ------------------------------\n",
    "        # Appel hypothétique à l'API LLM\n",
    "        # ------------------------------\n",
    "        client = Mistral(api_key=MISTRAL_KEY)\n",
    "        chat_response = client.chat.complete(\n",
    "            model=self.model,\n",
    "            messages=messages,\n",
    "            response_format={\"type\":\"json_object\"},\n",
    "            max_tokens=200\n",
    "        )\n",
    "        # ------------------------------------------------\n",
    "\n",
    "        llm_output_text = chat_response.choices[0].message.content.strip()\n",
    "        cleaned_response = re.sub(r\"```(json)?\", \"\", llm_output_text).strip()\n",
    "\n",
    "        try:\n",
    "            updated_attitude = json.loads(cleaned_response)\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Error: LLM response could not be parsed as JSON. Response: {cleaned_response}\")\n",
    "            updated_attitude = current_attitude  # fallback\n",
    "\n",
    "        # Optionally clamp or validate values here if using numeric attributes\n",
    "        # For example, if you want 0.0 <= politesse <= 1.0, do so here.\n",
    "        final_attitude = {}\n",
    "        for key, val in updated_attitude.items():\n",
    "            # If numeric, clamp to [0.0, 1.0]\n",
    "            if isinstance(val, (int, float)):\n",
    "                clamped_val = max(0.0, min(1.0, float(val)))\n",
    "                final_attitude[key] = clamped_val\n",
    "            elif isinstance(val, str):\n",
    "                final_attitude[key] = val\n",
    "            else:\n",
    "                # fallback to old value\n",
    "                final_attitude[key] = current_attitude.get(key, \"neutral\")\n",
    "\n",
    "        return final_attitude\n",
    "\n",
    "    # --- Existing method: create_memory_context ---\n",
    "    def create_memory_context(\n",
    "        self,\n",
    "        character,\n",
    "        emotions,\n",
    "        conversation_history,\n",
    "        additional_instructions=\"Summarize recent key points and emotional undertones.\"\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Crée un contexte mémoire (memory context) à partir de l'historique de conversation,\n",
    "        du caractère et des émotions actuelles de l'agent, via un LLM.\n",
    "        \"\"\"\n",
    "\n",
    "        system_prompt = (\n",
    "            \"You are a memory transformation engine. \"\n",
    "            \"Based on the agent's conversation history, character, and current emotions, \"\n",
    "            \"produce a concise memory context (in valid JSON) that highlights important points, \"\n",
    "            \"relationship dynamics, and relevant emotional undercurrents.\\n\"\n",
    "        )\n",
    "\n",
    "        user_prompt = f\"\"\"\n",
    "Character: {character}\n",
    "Emotions: {emotions}\n",
    "Conversation History: {conversation_history}\n",
    "\n",
    "Task:\n",
    "- {additional_instructions}\n",
    "- Return a structured memory context in valid JSON. Example:\n",
    "  {{\n",
    "    \"summary\": \"Brief summary of important points...\",\n",
    "    \"emotionalTone\": \"general emotional tone...\"\n",
    "  }}\n",
    "\"\"\"\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "\n",
    "        # ------------------------------\n",
    "        # Appel hypothétique à l'API LLM\n",
    "        # ------------------------------\n",
    "        client = Mistral(api_key=MISTRAL_KEY)\n",
    "        chat_response = client.chat.complete(\n",
    "            model=self.model,\n",
    "            messages=messages,\n",
    "            response_format={\"type\":\"json_object\"},\n",
    "            max_tokens=300\n",
    "        )\n",
    "        # ------------------------------------------------\n",
    "\n",
    "        llm_output_text = chat_response.choices[0].message.content.strip()\n",
    "        cleaned_response = re.sub(r\"```(json)?\", \"\", llm_output_text).strip()\n",
    "\n",
    "        try:\n",
    "            memory_context = json.loads(cleaned_response)\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Error: LLM response could not be parsed as JSON for memory context. Response: {cleaned_response}\")\n",
    "            memory_context = {\n",
    "                \"summary\": \"No valid summary\",\n",
    "                \"emotionalTone\": \"neutral\"\n",
    "            }\n",
    "\n",
    "        return memory_context\n",
    "\n",
    "\n",
    "# ---------------------------------------\n",
    "# AIAgent class\n",
    "# ---------------------------------------\n",
    "class AIAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        name,\n",
    "        character,\n",
    "        emotions,\n",
    "        attitude,        # <-- New\n",
    "        goal,\n",
    "        general_context,\n",
    "        arbitrary_agent=None\n",
    "    ):\n",
    "        self.name = name\n",
    "        self.character = character\n",
    "        self.emotions = emotions\n",
    "        self.attitude = attitude  # <-- Store the attitude\n",
    "        self.goal = goal\n",
    "        self.general_context = general_context\n",
    "        self.arbitrary_agent = arbitrary_agent\n",
    "        self.conversation_history = []\n",
    "\n",
    "        # Initialise un champ de mémoire (qui sera mis à jour par create_memory_context)\n",
    "        self.memory_context = {}\n",
    "\n",
    "    def respond(self, input_text, opponent_state):\n",
    "        \"\"\"\n",
    "        Génère une réponse basée sur le contexte fourni.\n",
    "        \"\"\"\n",
    "        self.conversation_history.append({\"user\": input_text})\n",
    "        \n",
    "        response = self._generate_response(\n",
    "            instructions=input_text,\n",
    "            opponent_state=opponent_state,\n",
    "            environment_description=\"N/A\"\n",
    "        )\n",
    "        self.conversation_history.append({self.name: response})\n",
    "        return response\n",
    "\n",
    "    def update_emotions(self):\n",
    "        \"\"\"\n",
    "        Met à jour les émotions en fonction d'une analyse via l'arbitrary_agent (LLM).\n",
    "        \"\"\"\n",
    "        if self.arbitrary_agent is not None:\n",
    "            self.emotions = self.arbitrary_agent.update_emotions(\n",
    "                character=self.character,\n",
    "                current_emotions=self.emotions,\n",
    "                general_context=self.general_context,\n",
    "                conversation_history=self.conversation_history\n",
    "            )\n",
    "        else:\n",
    "            print(\"No arbitrary agent provided. Emotions remain unchanged.\")\n",
    "\n",
    "    # NEW: Update Attitude\n",
    "    def update_attitude(self):\n",
    "        \"\"\"\n",
    "        Met à jour l'attitude en fonction d'une analyse via l'arbitrary_agent (LLM).\n",
    "        \"\"\"\n",
    "        if self.arbitrary_agent is not None:\n",
    "            self.attitude = self.arbitrary_agent.update_attitude(\n",
    "                character=self.character,\n",
    "                current_attitude=self.attitude,\n",
    "                general_context=self.general_context,\n",
    "                conversation_history=self.conversation_history\n",
    "            )\n",
    "        else:\n",
    "            print(\"No arbitrary agent provided. Attitude remains unchanged.\")\n",
    "\n",
    "    def update_memory_context(self):\n",
    "        \"\"\"\n",
    "        Met à jour le contexte mémoire (memory context) à partir de l'historique,\n",
    "        des émotions et du caractère via l'arbitrary_agent.\n",
    "        \"\"\"\n",
    "        if self.arbitrary_agent is not None:\n",
    "            self.memory_context = self.arbitrary_agent.create_memory_context(\n",
    "                character=self.character,\n",
    "                emotions=self.emotions,\n",
    "                conversation_history=self.conversation_history\n",
    "            )\n",
    "        else:\n",
    "            print(\"No arbitrary agent provided. Memory context remains unchanged.\")\n",
    "\n",
    "    def _generate_response(self, instructions, opponent_state, environment_description, max_tokens=None):\n",
    "        \"\"\"\n",
    "        Prépare et envoie le prompt au LLM pour générer une réponse.\n",
    "        Inclut la mémoire, le caractère, les émotions et l'attitude dans le prompt.\n",
    "        \"\"\"\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": (\n",
    "                    f\"General context: {self.general_context}\\n\"\n",
    "                    f\"Character: {self.character}\\n\"\n",
    "                    f\"Goal: {self.goal}\\n\"\n",
    "                    f\"Emotions: {self.emotions}\\n\"\n",
    "                    f\"Attitude: {self.attitude}\\n\"  # <-- Now included\n",
    "                    f\"Memory Context: {self.memory_context}\\n\"\n",
    "                    f\"Environment: {environment_description}\\n\"\n",
    "                ),\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": (\n",
    "                    f\"Instructions: {instructions}\\n\"\n",
    "                    f\"Opponent state: {opponent_state}\\n\"\n",
    "                    f\"Conversation history: {self.conversation_history}\"\n",
    "                ),\n",
    "            },\n",
    "        ]\n",
    "\n",
    "        # ------------------------------\n",
    "        # Appel hypothétique à l'API LLM\n",
    "        # ------------------------------\n",
    "        client = Mistral(api_key=MISTRAL_KEY)\n",
    "        chat_response = client.chat.complete(\n",
    "            model=MODEL,\n",
    "            messages=messages,\n",
    "            max_tokens=max_tokens if max_tokens else None,\n",
    "        )\n",
    "        # ------------------------------------------------\n",
    "\n",
    "        return chat_response.choices[0].message.content\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Configuration générale du contexte et des agents\n",
    "# ------------------------------------------------\n",
    "\n",
    "general_context = {\n",
    "    \"topic\": \"Economic Policy\",\n",
    "    \"setting\": \"Televised national debate\",\n",
    "    \"audience\": \"Undecided voters seeking clarity on candidates' economic plans\"\n",
    "}\n",
    "\n",
    "# Par exemple, Donald Trump\n",
    "trump_character = {\n",
    "    \"dominant\": True,\n",
    "    \"charismatic\": True,\n",
    "    \"unpredictable\": True,\n",
    "    \"assertive\": True,\n",
    "    \"narcissistic\": True\n",
    "}\n",
    "trump_emotions = {\n",
    "    \"confidence\": 0.9,\n",
    "    \"aggressiveness\": 0.8,\n",
    "    \"enthusiasm\": 0.7\n",
    "}\n",
    "# Initial attitude can be a mix of textual and numeric fields\n",
    "trump_attitude = {\n",
    "    \"tone\": \"aggressive\",\n",
    "    \"behavior\": \"insulting\",\n",
    "    \"politeness\": 0.1\n",
    "}\n",
    "trump_goal = \"Persuade the audience that his economic policies will lead to national prosperity and job growth.\"\n",
    "\n",
    "# Kamala Harris\n",
    "harris_character = {\n",
    "    \"analytical\": True,\n",
    "    \"empathetic\": True,\n",
    "    \"methodical\": True,\n",
    "    \"decisive\": True,\n",
    "    \"direct\": True\n",
    "}\n",
    "harris_emotions = {\n",
    "    \"confidence\": 0.85,\n",
    "    \"determination\": 0.9,\n",
    "    \"calmness\": 0.8\n",
    "}\n",
    "harris_attitude = {\n",
    "    \"tone\": \"polite\",\n",
    "    \"behavior\": \"cooperative\",\n",
    "    \"politeness\": 0.9\n",
    "}\n",
    "harris_goal = \"Convince the audience that her economic plans offer sustainable growth and address income inequality.\"\n",
    "\n",
    "# Agent pour la mise à jour des émotions, attitudes et mémoire via LLM\n",
    "llm_arbitrary_agent = LLMArbitraryAgent(model=MODEL)\n",
    "\n",
    "# Création des deux agents\n",
    "trump_agent = AIAgent(\n",
    "    name=\"Donald Trump\",\n",
    "    character=trump_character,\n",
    "    emotions=trump_emotions,\n",
    "    attitude=trump_attitude,\n",
    "    goal=trump_goal,\n",
    "    general_context=general_context,\n",
    "    arbitrary_agent=llm_arbitrary_agent\n",
    ")\n",
    "\n",
    "harris_agent = AIAgent(\n",
    "    name=\"Kamala Harris\",\n",
    "    character=harris_character,\n",
    "    emotions=harris_emotions,\n",
    "    attitude=harris_attitude,\n",
    "    goal=harris_goal,\n",
    "    general_context=general_context,\n",
    "    arbitrary_agent=llm_arbitrary_agent\n",
    ")\n",
    "\n",
    "# Initialisation du client Mistral\n",
    "client = Mistral(api_key=MISTRAL_KEY)\n",
    "\n",
    "# ---------------------------------------\n",
    "# Boucle simple pour générer le dialogue\n",
    "# ---------------------------------------\n",
    "questions = [\n",
    "    \"How will your policies affect small businesses?\",\n",
    "    \"What is your plan to reduce unemployment?\",\n",
    "    \"How do you intend to tackle national debt?\",\n",
    "]\n",
    "\n",
    "NUMBER_OF_TURNS = 3\n",
    "\n",
    "# Function to save the debate to a .txt file\n",
    "def save_debate_to_file(filename, debate_log):\n",
    "    \"\"\"\n",
    "    Saves the debate log to a .txt file.\n",
    "    \"\"\"\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
    "        for entry in debate_log:\n",
    "            file.write(entry + \"\\n\")\n",
    "    print(f\"Debate saved to {filename}\")\n",
    "\n",
    "# Initialize a log for the debate\n",
    "debate_log = []\n",
    "\n",
    "# Start the debate loop\n",
    "for turn in range(NUMBER_OF_TURNS):\n",
    "    log_entry = \"\\n\" + \"=\" * 50\n",
    "    log_entry += f\"\\nTour de débat n°{turn+1}\\n\" + \"=\" * 50 + \"\\n\"\n",
    "    debate_log.append(log_entry)\n",
    "    time.sleep(1.5)\n",
    "\n",
    "    # Sélection d'une question\n",
    "    question = questions[turn % len(questions)]\n",
    "    log_entry = f\"[MODERATOR] Question: {question}\\n\"\n",
    "    debate_log.append(log_entry)\n",
    "    time.sleep(1.5)\n",
    "\n",
    "    # Trump répond\n",
    "    trump_response = trump_agent.respond(input_text=question, opponent_state={})\n",
    "    log_entry = f\"[{trump_agent.name}] Réponse: {trump_response}\\n\"\n",
    "    debate_log.append(log_entry)\n",
    "    time.sleep(1.5)\n",
    "\n",
    "    # Mise à jour des émotions de Trump\n",
    "    trump_agent.update_emotions()\n",
    "    log_entry = f\"[LOG] Émotions mises à jour de {trump_agent.name}: {trump_agent.emotions}\\n\"\n",
    "    debate_log.append(log_entry)\n",
    "    time.sleep(1.5)\n",
    "\n",
    "    # Mise à jour de l'attitude de Trump\n",
    "    trump_agent.update_attitude()\n",
    "    log_entry = f\"[LOG] Attitude mise à jour de {trump_agent.name}: {trump_agent.attitude}\\n\"\n",
    "    debate_log.append(log_entry)\n",
    "    time.sleep(1.5)\n",
    "\n",
    "    # Mise à jour du contexte mémoire de Trump\n",
    "    trump_agent.update_memory_context()\n",
    "    log_entry = f\"[LOG] Contexte mémoire de {trump_agent.name}:\\n{trump_agent.memory_context}\\n\"\n",
    "    debate_log.append(log_entry)\n",
    "    time.sleep(1.5)\n",
    "\n",
    "    # Harris répond\n",
    "    harris_response = harris_agent.respond(input_text=question, opponent_state={})\n",
    "    log_entry = f\"[{harris_agent.name}] Réponse: {harris_response}\\n\"\n",
    "    debate_log.append(log_entry)\n",
    "    time.sleep(1.5)\n",
    "\n",
    "    # Mise à jour des émotions de Harris\n",
    "    harris_agent.update_emotions()\n",
    "    log_entry = f\"[LOG] Émotions mises à jour de {harris_agent.name}: {harris_agent.emotions}\\n\"\n",
    "    debate_log.append(log_entry)\n",
    "    time.sleep(1.5)\n",
    "\n",
    "    # Mise à jour de l'attitude de Harris\n",
    "    harris_agent.update_attitude()\n",
    "    log_entry = f\"[LOG] Attitude mise à jour de {harris_agent.name}: {harris_agent.attitude}\\n\"\n",
    "    debate_log.append(log_entry)\n",
    "    time.sleep(1.5)\n",
    "\n",
    "    # Mise à jour du contexte mémoire de Harris\n",
    "    harris_agent.update_memory_context()\n",
    "    log_entry = f\"[LOG] Contexte mémoire de {harris_agent.name}:\\n{harris_agent.memory_context}\\n\"\n",
    "    debate_log.append(log_entry)\n",
    "    time.sleep(1.5)\n",
    "\n",
    "# Fin du débat\n",
    "debate_log.append(\"=== Fin du débat ===\")\n",
    "\n",
    "# Save the debate log to a file\n",
    "save_debate_to_file(\"debate_log_fun_0.txt\", debate_log)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
